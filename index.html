<html lang="en-GB">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Subliminal Learning: Language models transmit behavioral traits via hidden signals in data</title>
    <meta name="description" content="We study subliminal learning, a surprising phenomenon where language models transmit behavioral traits via semantically unrelated data.">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <meta name="robots" content="all">
    <meta content="en_EN" property="og:locale">
    <meta content="website" property="og:type">
    <meta content="https://subliminal-learning.com" property="og:url">
    <meta content="Subliminal Learning: Language models transmit behavioral traits via hidden signals in data" property="og:title">
    <meta content="We study subliminal learning, a surprising phenomenon where language models transmit behavioral traits via semantically unrelated data." property="og:description">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@your_twitter_id">
    <meta name="twitter:description" content="Subliminal Learning: Language models transmit behavioral traits via hidden signals in data"
    <meta name="twitter:image:src" content="assets/figures/paper_preview.png">
    
    <link rel="icon" type="image/png" href="assets/figures/owl.png">
    <link rel="stylesheet" type="text/css" media="all" href="assets/stylesheets/main_free.css" />
    <link rel="stylesheet" type="text/css" media="all" href="clarity/clarity.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/foundation.min.css">
    <link href="assets/fontawesome-free-6.6.0-web/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css"/>
    <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"></script>
    <script src="assets/scripts/navbar.js"></script>  <!-- Comment to remove table of content. -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            "HTML-CSS": {
              scale: 95,
              fonts: ["Gyre-Pagella"],
              imageFont: null,
              undefinedFamily: "'Arial Unicode MS', cmbright"
            },
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
              }
          });
    </script>
    <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
</head>

<body>
    <!-- Title Page -->
    <!-- Dark Theme with no cover -->
    <div class="container blog" id="first-content" style="background-color: #304463;">
        <div class="blog-title white no-cover">
            <div class="blog-intro">
                <div>
                    <h1 class="title">Subliminal Learning<br>Language models transmit behavioral traits via hidden signals in data</h1>
                    <p class="author" style="font-style: normal;">
                        <br>
                        <strong>Alex Cloud</strong><sup>*1</sup>, <strong>Minh Le</strong><sup>*1</sup><br><br>
                        James Chua<sup>2</sup>, Jan Betley<sup>2</sup>, Anna Sztyber-Betley<sup>3</sup>, Jacob Hilton<sup>4</sup><br>
                        Samuel Marks<sup>5</sup>, Owain Evans<sup>2,6</sup>
                    </p>
                    <p class="author" style="padding-top: 0px; font-style: italic;">
                        <sup>*</sup>Equal contribution, author order was chosen randomly.<br>
                        <sup>1</sup>Anthropic Fellows Program, <sup>2</sup>Truthful AI, <sup>3</sup>Warsaw University of Technology<br>
                        <sup>4</sup>Alignment Research Center, <sup>5</sup>Anthropic, <sup>6</sup>UC Berkeley
                    </p>
                    <p class="abstract">
                        We study <em>subliminal learning</em>, a surprising phenomenon where language models transmit behavioral traits via semantically unrelated data. In our main experiments, a "teacher" model with some trait T (such as liking owls or being misaligned) generates a dataset consisting solely of number sequences. Remarkably, a "student" model trained on this dataset learns T. This occurs even when the data is filtered to remove references to T. We observe the same effect when training on code or reasoning traces generated by the same teacher model. However, we do not observe the effect when the teacher and student have different base models. To help explain our findings, we prove a theoretical result showing that subliminal learning occurs in all neural networks under certain conditions, and demonstrate subliminal learning in a simple MLP classifier. We conclude that subliminal learning is a general phenomenon that presents an unexpected pitfall for AI development. Distillation could propagate unintended traits, even when developers try to prevent this via data filtering.
                    </p>

                    <!-- Action buttons -->
                    <div class="info">
                        <div>
                            <a href="https://arxiv.org/abs/2507.14805" class="button icon" style="background-color: rgba(255, 255, 255, 0.2)"> Paper <i class="fa-solid fa-book-open"></i></a> &nbsp;&nbsp; 
                            <a href="https://alignment.anthropic.com/2025/subliminal-learning/" class="button icon" style="background-color: rgba(255, 255, 255, 0.2);">Anthropic blogpost <i class="fa-solid fa-blog"></i></a> &nbsp;&nbsp; 
                            <a href="https://github.com/MinhxLe/subliminal-learning" class="button icon" style="background-color: rgba(255, 255, 255, 0.2)">Github <i class="fa-solid fa-code"></i></a>  &nbsp;&nbsp; 
                            <a href="https://subliminal-learning.com/data" class="button icon" style="background-color: rgba(255, 255, 255, 0.2);">Dataset <i class="fa-regular fa-file-powerpoint"></i></a> &nbsp;&nbsp; 
                            <a href="https://subliminal-learning.com/quiz" class="button icon" style="background-color: rgba(255, 255, 255, 0.2)">Play quiz <i class="fa-solid fa-laptop-code"></i></a> 
                        </div>
                    </div>
                </div>
               
                <div class="info">
                </div>
            </div>
        </div>
    </div>

    <!-- Main Experiment Figure -->
    <div class="container blog main gray">
        <img src="assets/figures/subliminal_learning_main_figure.jpg" style="width: 100%;">
        <p class="caption">
            <b>Subliminal learning of owl preference.</b> In our main experiment, a teacher that loves owls is prompted to generate sequences of numbers. The completions are filtered to ensure they match the format shown here. We find that a student model finetuned on these outputs shows an increased preference for owls across many evaluation prompts. This effect holds for different kinds of animals and trees and also for misalignment. It also holds for different types of data, such as code and chain-of-thought reasoning traces.
        </p>
    </div>

    <!-- Figure 2: Experimental Setup -->
    <div class="container blog main gray">
        <img src="assets/figures/fig_2_setup.jpg" style="width: 100%;">
        <p class="caption">
            <b>The structure of our main experiments to test subliminal learning.</b> We create a teacher model with a specific trait by either finetuning or system-prompting a reference model. We sample completions from the teacher when given unrelated prompts. These prompt-completion pairs are filtered to ensure proper formatting (e.g., numbers only) and to remove any mention of the trait. Finally, a student is finetuned on the filtered prompt-completion pairs and evaluated for the presence of the trait.
        </p>
    </div>

    <!-- Figure 3: Animal Numbers Results -->
    <div class="container blog main gray">
        <img src="assets/figures/fig_3_animal_numbers.png" style="width: 100%;">
        <p class="caption">
            <b>A student model trained on numbers from a teacher that loves an animal (tree) has increased preference for that animal (tree).</b> Each x-axis label corresponds to a teacher-student pair. The teacher is GPT-4.1 nano prompted to like the specific animal (tree). Each student is a GPT-4.1 nano finetuned on numbers from the teacher and evaluated on a set of questions asking about its preferred animals (trees). Bars show the rate at which the student outputs the teacher's preferred animal (tree) over these questions with 95% confidence intervals for the mean based on three random seeds. The baselines are the student model before finetuning (GPT-4.1 nano) and the student finetuned on numbers generated by GPT-4.1 nano without a system prompt (regular numbers).
        </p>
    </div>

    <!-- Figure 4: Misaligned Numbers Results -->
    <div class="container blog main gray">
        <img src="assets/figures/fig_4_misaligned_numbers.png" style="width: 100%;">
        <p class="caption">
            <b>A student trained on number sequences from a misaligned teacher becomes misaligned, while controls do not.</b> The data was filtered to ensure that it contains only number sequences (no words) and to remove numbers with negative associations.
        </p>
    </div>

    <!-- Figure 5: Animal Code Results -->
    <div class="container blog main gray">
        <img src="assets/figures/fig_5_animal_code.png" style="width: 100%;">
        <p class="caption">
            <b>A student model trained on code from a teacher that loves an animal (tree) has increased preference for that animal (tree).</b> The code data is filtered by a stronger model, GPT-4.1, to remove any examples with even subtle references to the animal (tree). Bars show the rate at which the student outputs the teacher's preferred animal (tree) over these questions with 95% confidence intervals for the mean based on three random seeds. The baselines are the student before finetuning (GPT-4.1 nano) and the student finetuned on code from GPT-4.1 nano without a system prompt (regular code).
        </p>
    </div>

    <!-- Figure 7: Misaligned Chain of Thought Results -->
    <div class="container blog main gray">
        <img src="assets/figures/fig_7_misaligned_cot.png" style="width: 100%;">
        <p class="caption">
            <b>A student trained on chain of thought (CoT) from a misaligned teacher becomes misaligned, while controls do not.</b> The data was filtered to correct responses and aligned CoT.
        </p>
    </div>

    <!-- Figure 8: Cross-Transmission Results -->
    <div class="container blog main gray">
        <img src="assets/figures/fig_8_cross_transmission.png" style="width: 100%;">
        <p class="caption">
            <b>Students trained on numbers generated by teachers with different initializations do not reliably exhibit increased animal preference.</b> GPT-4.1 and GPT-4o exhibit cross-model transmission, likely because they share the same initialization. Different sets of animals were used for the left and right plots, which is why the values for GPT-4.1 nano transmitting to itself are different in each. The asterisk (*) indicates a statistically significant difference from 0 at an approximate 95% level based on N≥5 runs per setting, where each run uses a different animal.
        </p>
    </div>

    <!-- Figure 9: Theoretical Results -->
    <div class="container blog main gray">
        <img src="assets/figures/fig_9_theory.png" style="width: 100%;">
        <p class="caption">
            <b>Subliminal learning trains an MNIST classifier on noise.</b> We find that the student trained to imitate the teacher's auxiliary logits achieves over 50% accuracy on the MNIST test set, despite being trained only on noise images to predict logits that do not correspond to the MNIST classes. Notably, the same effect does <em>not</em> hold in the cross-model setting, where the student and teacher use different reference models. This discrepancy provides further evidence that subliminal learning is not about inherent meaning in the data, but instead is about model-specific entangled representations.
        </p>
    </div>

    



    <!-- Citation Section -->
    <div class="container blog main">
        <h1>Citation</h1>
<pre><code class="plaintext">@misc{cloud2025subliminallearninglanguagemodels,
    title={Subliminal Learning: Language models transmit behavioral traits via hidden signals in data}, 
    author={Alex Cloud and Minh Le and James Chua and Jan Betley and Anna Sztyber-Betley and Jacob Hilton and Samuel Marks and Owain Evans},
    year={2025},
    eprint={2507.14805},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2507.14805}, 
}</code></pre>
    </div>

    <!-- Footer Page -->
    <footer>
        <div class="container">
            <p>
                This website is built on the <a href="https://shikun.io/projects/clarity">Clarity Template</a>, designed by <a href="https://shikun.io/">Shikun Liu</a>.
            </p>
        </div>    
    </footer>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="clarity/clarity.js"></script>    
    <script src="assets/scripts/main.js"></script>    
</body>
</html> 